{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "name": "Homework_5_part2_GCP.ipynb",
   "provenance": [],
   "collapsed_sections": [],
   "toc_visible": true
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7tV6wYMl7ype"
   },
   "source": [
    "# ECE 795 - Big Data\n",
    "## Homework_5_part2 - Twitter Word Count using PySpark Distributed Computing on DataProc (Due 3/31, 40 Total Points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P2dz95IU8QJb"
   },
   "source": [
    "### Provide your credentials to the runtime"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "pZrs8Onl5Vns"
   },
   "source": [
    "# Authenticate your student profile\n",
    "\n",
    "from google.colab import auth\n",
    "auth.authenticate_user()\n",
    "print('Authenticated')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "48KU9aLS8Yd5"
   },
   "source": [
    "### Set the Project ID and Enable APIs"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "sUg7m-aj8Rr6"
   },
   "source": [
    "project_id = 'ece795-xxxxx'"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E4v97QS235RN"
   },
   "source": [
    "#### In GCP, there are many different services; Compute Engine, Cloud Storage, BigQuery, Cloud SQL, Cloud Dataproc to name a few. In order to use any of these services in your project, you first have to enable them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-XAex6YX3_Es"
   },
   "source": [
    "![alt text](https://cdn-images-1.medium.com/max/1600/1*rYZZH8w9iScxIXG27qG-ww.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MJnvIcKk4Ldc"
   },
   "source": [
    "#### Put your mouse over “APIs & Services” on the left-side menu, then click into “Library”. For this project, we will enable three APIs: Cloud Dataproc, Compute Engine, and Cloud Storage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vz06iAlA4Grc"
   },
   "source": [
    "![alt text](https://cdn-images-1.medium.com/max/1600/1*qH5u_JSH2JLZW_SQTcetSQ.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fev7JQYYoje6"
   },
   "source": [
    "### Running Example 1: Word Count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N7VZj2MqqcmD"
   },
   "source": [
    "#### This word count example will use the Shakespeare dataset in BigQuery. The only difference is that instead of using Hadoop, it uses PySpark which is a Python library for Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V4E_-MsuqkvH"
   },
   "source": [
    "Step 1: create a dataset named \"wordcount_dataset\"\n",
    "- Select your project or create a new one, remember to enable billing\n",
    "- Go to Big Query\n",
    "- Create a dataset, and name it wordcount_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2CZUAdrPyx_h"
   },
   "source": [
    "![alt text](https://github.com/msaadsadiq/BigDataCourse/blob/master/Assign3_img11.png?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rg4RWPWArcrl"
   },
   "source": [
    "#### Step 2: create a cluster in Dataproc and Google Cloud Storage. Go to Dataproc and create a cluster similar to our previous tutorial. It should look like this"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0T3sfBOxrfU-"
   },
   "source": [
    "![alt text](https://github.com/msaadsadiq/BigDataCourse/blob/master/Assgn3_img2.png?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PiZr43X2rxeS"
   },
   "source": [
    "Now click on the link in the “Cloud Storage staging bucket” column, it will bring you to your Cloud\n",
    "Storage. This is where we will upload our python code, which will then give us a link to submit a job to the cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "igwEA0oOrzmc"
   },
   "source": [
    "#### Step 3: modify the code and upload it to Cloud Storage. Download the following code from here (Fill word count Spark code by yourself)\n",
    "\n",
    "Change the input and output directory strings. Replace the {} symbols with your Cloud Storage Bucket id.\n",
    "\n",
    "input_directory =\n",
    "'gs://{}/hadoop/tmp/bigquery/pyspark_input'.\n",
    "output_directory =\n",
    "\n",
    "'gs://{}/hadoop/tmp/bigquery/pyspark_output'\n",
    "\n",
    "Upload the python file to your Cloud Storage. We will use this link as input to our PySpark job."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "I8eQM0BGoqmi"
   },
   "source": [
    "#!/usr/bin/python\n",
    "\"\"\"BigQuery I/O PySpark example.\"\"\"\n",
    "from __future__ import absolute_import\n",
    "import json\n",
    "import pprint\n",
    "import subprocess\n",
    "import pyspark\n",
    "from pyspark.sql import SQLContext\n",
    "\n",
    "sc = pyspark.SparkContext()\n",
    "\n",
    "# Use the Cloud Storage bucket for temporary BigQuery export data used\n",
    "# by the InputFormat. This assumes the Cloud Storage connector for\n",
    "# Hadoop is configured.\n",
    "bucket = sc._jsc.hadoopConfiguration().get('fs.gs.system.bucket')\n",
    "project = sc._jsc.hadoopConfiguration().get('fs.gs.project.id')\n",
    "input_directory = 'gs://{}/hadoop/tmp/bigquery/pyspark_input'.format(bucket)\n",
    "\n",
    "conf = {\n",
    "    # Input Parameters.\n",
    "    'mapred.bq.project.id': project,\n",
    "    'mapred.bq.gcs.bucket': bucket,\n",
    "    'mapred.bq.temp.gcs.path': input_directory,\n",
    "    'mapred.bq.input.project.id': 'publicdata',\n",
    "    'mapred.bq.input.dataset.id': 'samples',\n",
    "    'mapred.bq.input.table.id': 'shakespeare',\n",
    "}\n",
    "\n",
    "# Output Parameters.\n",
    "output_dataset = 'wordcount_dataset'\n",
    "output_table = 'wordcount_output'\n",
    "\n",
    "# Load data in from BigQuery.\n",
    "table_data = sc.newAPIHadoopRDD(\n",
    "    'com.google.cloud.hadoop.io.bigquery.JsonTextBigQueryInputFormat',\n",
    "    'org.apache.hadoop.io.LongWritable',\n",
    "    'com.google.gson.JsonObject',\n",
    "    conf=conf)\n",
    "\n",
    "# Perform word count.\n",
    "\n",
    "\n",
    "################################################################################\n",
    "# Fill by yourself\n",
    "################################################################################\n",
    "\n",
    "\n",
    "# Display 10 results.\n",
    "pprint.pprint(word_counts.take(10))\n",
    "\n",
    "# Stage data formatted as newline-delimited JSON in Cloud Storage.\n",
    "output_directory = 'gs://{}/hadoop/tmp/bigquery/pyspark_output'.format(bucket)\n",
    "output_files = output_directory + '/part-*'\n",
    "\n",
    "sql_context = SQLContext(sc)\n",
    "(word_counts\n",
    " .toDF(['word', 'word_count'])\n",
    " .write.format('json').save(output_directory))\n",
    "\n",
    "# Shell out to bq CLI to perform BigQuery import.\n",
    "subprocess.check_call(\n",
    "    'bq load --source_format NEWLINE_DELIMITED_JSON '\n",
    "    '--replace '\n",
    "    '--autodetect '\n",
    "    '{dataset}.{table} {files}'.format(\n",
    "        dataset=output_dataset, table=output_table, files=output_files\n",
    "    ).split())\n",
    "\n",
    "# Manually clean up the staging_directories, otherwise BigQuery\n",
    "# files will remain indefinitely.\n",
    "input_path = sc._jvm.org.apache.hadoop.fs.Path(input_directory)\n",
    "input_path.getFileSystem(sc._jsc.hadoopConfiguration()).delete(input_path, True)\n",
    "output_path = sc._jvm.org.apache.hadoop.fs.Path(output_directory)\n",
    "output_path.getFileSystem(sc._jsc.hadoopConfiguration()).delete(\n",
    "    output_path, True)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F_tMGQjoshl1"
   },
   "source": [
    "![alt text](https://github.com/msaadsadiq/BigDataCourse/blob/master/Assgn3_img3.png?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gj7HDcclt5dw"
   },
   "source": [
    "For example, here I uploaded sparkwc.py, and my link would be gs://dataproc-e4225d08-23a8-481faff9-5205024119a5-us/sparkwc.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o43Pbiatt7wX"
   },
   "source": [
    "#### Step 4: submit the job. Similar to our Hadoop example. Use the settings below. Remember to use your own gs:// link, not mine. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mbmw6HxQuAFm"
   },
   "source": [
    "![alt text](https://github.com/msaadsadiq/BigDataCourse/blob/master/Assgn3_img12.png?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fgCZ0uJtyRlq"
   },
   "source": [
    "#### Step 5: browse the result. Once the job is done (should be around 2~10 minutes). Go back to Big Query and explore the result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gt31KropyUq6"
   },
   "source": [
    "![alt text](https://github.com/msaadsadiq/BigDataCourse/blob/master/Assgn3_img5.png?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hzXd1cXAynDK"
   },
   "source": [
    " #### Reference. \n",
    "https://cloud.google.com/dataproc/docs/tutorials/bigquery-connector-spark-example\n",
    "https://piazza.com/class_profile/get_resource/j1318wb5md21wb/j2jthscnpkb5zu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iE7RT7LipZQ7"
   },
   "source": [
    "### Question 1. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qcEh6qpmpS1s"
   },
   "source": [
    "1. Download 10,000 Tweets about any topic you like (english language) from https://data.world/datasets/twitter\n",
    "2. Save them as csv and Upload Tweets to storage bucket\n",
    "3. Using PySpark to import those tweets to rdd or dataframe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yKQJ_yh0pRDM"
   },
   "source": [
    "### Question 2. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "08WpxyGkpeC9"
   },
   "source": [
    "1. Perform a total word count on your tweets.\n",
    "2. Perform a word count for each type of the words on your tweets."
   ]
  }
 ]
}