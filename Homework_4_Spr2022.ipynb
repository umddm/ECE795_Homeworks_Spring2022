{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Homework_4_Spr2022.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sq8U3BtmhtRx"
      },
      "source": [
        "\n",
        "# ECE795 Advanced Big Data Analytics Homework 4 (80 Points + 20 Bonus Points) Due Date: March 8, 2022 (by 1:00pm)\n",
        "\n",
        "Set up Pyspark Environment.\n",
        "\n",
        "Tips for Colab:\n",
        "\n",
        "1. You will be disconnected if you are idle for more than 90 minutes and will be mandatorily disconnected after 12 hour connection. \n",
        "\n",
        "2. Once you got disconnected, you need to execute the codes from the beginning to setup the environment again.\n",
        "\n",
        "3. For the purpose of homework, it should be sufficient since each problem should not take more than 5 minutes to generate the results.\n",
        "\n",
        "4. To facilitate the use of Colab, you can use \"MainMenu - Runtime - Run all” to run all the cells in the notebook. So you do not have to click each cell to setup the environment."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lh5NCoc8fsSO"
      },
      "source": [
        "!pip install pyspark\n",
        "!pip install -q findspark\n",
        "\n",
        "import findspark\n",
        "findspark.init()\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JEb4HTRwiaJx"
      },
      "source": [
        "Congrats! Your Colab is ready to run Pyspark.\n",
        "\n",
        "# Read input text file to RDD \n",
        "\n",
        "Download the input data from [here](https://raw.githubusercontent.com/umddm/ECE795_Homeworks/master/clust_B.txt)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PAISFqHXf7dt"
      },
      "source": [
        "!wget https://raw.githubusercontent.com/umddm/ECE795_Homeworks_Spring2022/homework_4/StudentsPerformance.csv"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "21D9EANUvnwF"
      },
      "source": [
        "Now that we have input data, we can start to do the homework. \n",
        "\n",
        "## Question 1: Please rewrite the given function only with the limited operation. (15 Points)\n",
        "\n",
        "### Expected output:\n",
        "```\n",
        ".sum() by .map() and .reduce().\n",
        "\n",
        "can be rewritten as .reduce(lambda a, b: a + b)\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ZeJ7WQCgM8g"
      },
      "source": [
        "#sc.textFile\n",
        "from pyspark import SparkConf, SparkContext\n",
        "sc = SparkContext.getOrCreate()\n",
        "\n",
        "#Sample\n",
        "Sample = sc.textFile('StudentsPerformance.csv')\n",
        "Sample = Sample.map(lambda x: x.replace('\"', ''))\n",
        "header = Sample.first()\n",
        "Sample = Sample.filter(lambda line: line != header)\n",
        "Sample.map(lambda x: int(x.split(',')[-1])).sum() == Sample.map(lambda x: int(x.split(',')[-1])).reduce(lambda a, b: a + b) \n",
        "#Output is Ture\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2xSGwsLnr1th"
      },
      "source": [
        "### Question 1. 1: (5 Points)\n",
        "Please implement .count() by .map() and .reduce()."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kFcAP7ulr8FN"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EFaBf2J0sWcn"
      },
      "source": [
        "### Question 1. 2: (5 Points)\n",
        "Please implement .stdev() by .map() and .reduce().\n",
        "\n",
        "Round to the second decimal places"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HYE_O6ch_P7n"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T7Bog6GoDrp1"
      },
      "source": [
        "### Question 1. 3: (5 Points)\n",
        "Please implement .reduceByKey() by partitionBy(), map() and glom()\n",
        "\n",
        "A hint to solve this problem is that a custom partitioner should be implemented and use glom to group the data.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E3msZIoeER3y"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y3vYyp5dwOm_"
      },
      "source": [
        "## Question 2: Please create an RDD, which contains range(12) with 3 partitions. Then count the minimum of the partition 1, the maximum of the partition 2, and the sum of the third partition. (15 Points)\n",
        "\n",
        "\n",
        "### Sample:\n",
        "```\n",
        "min of the partition 1 = 1\n",
        "max of the partition 2 = 10\n",
        "sum of the partition 3 = 20\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eja1BLiaTThT"
      },
      "source": [
        "#Question_2\n",
        "\n",
        "#Fill out here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6FHVHVQjd4P1"
      },
      "source": [
        "## Question 3: Why will pre-partition benefit spark job? Please write a short explanation and a code to prove your answer. (20 Points)\n",
        "\n",
        "### Hint:\n",
        "`ToDebugString()` can be used to debug. Detailed description can be found at https://spark.apache.org/docs/latest/api/java/org/apache/spark/rdd/RDD.html#toDebugString--\n",
        "\n",
        "You can use `toDebugString()` like below:\n",
        "```\n",
        "rdd = sc.textFile('clust_B_part1.txt').toDebugString() \n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MJsxix2Xd3-Q"
      },
      "source": [
        "#Question_3\n",
        "\n",
        "#Fill out here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uIy1IL965Ogm"
      },
      "source": [
        "## Question 4: Please implement the K-Means algorithm described in the lecture slide. And test the implemented algorithm with data in \"s1.txt\" (k=15). (30 Points)\n",
        "\n",
        "\n",
        "The coordinates of the center change less than 0.1 are considered as unchanged.\n",
        "\n",
        "### Input data format:\n",
        "Each row describe a point using two integers, which are its x and y coordinates.\n",
        "The ID of the points is the row number, i.e., the point in the first row has ID=1, the point in the second row has ID=2, etc.\n",
        "\n",
        "### Expected output format:\n",
        "Each line describes a cluster including the cluster ID and the coordinate of its center\n",
        "```\n",
        "Cluster 1 (12.00, 67.33)\n",
        "Cluster 2 (33.25, 23.02)\n",
        "...\n",
        "Cluster 15 (123.43, 456.30)\n",
        "```\n",
        "\n",
        "### Hint:\n",
        "`zipWithIndex()` can be used to add index to the rdd. Detailed description can be found at https://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.zipWithIndex"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5FrRFR9pVQ8L"
      },
      "source": [
        "# Please download the data using the following commands\n",
        "!wget http://cs.joensuu.fi/sipu/datasets/s1.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7OmbLiCQ5ctu"
      },
      "source": [
        "#Question_4\n",
        "\n",
        "#Fill out here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_IYg8pSLUkxj"
      },
      "source": [
        "## Question 5: Please implement the One-Dimension Gaussian Mixture model described in the following. And test the implemented algorithm with the **first column** of data in \"s1.txt\" (k = 15). (Bonus: 20 Points)\n",
        "\n",
        "\n",
        "No matter whether the model converges, run the model for 10 iterations\n",
        "\n",
        "### Algorithm Description\n",
        "\n",
        "This algorithm is similar to K-Means, clustering the given data. Different from K-means, Gaussain Mixture model defines each cluster by mean, variance and weight, instead of the coordination of the cluster center\n",
        "\n",
        "- Step1: Assign initial means, variance, and weights of each cluster\n",
        "\n",
        "  - Each cluster is defined by three value $C(\\mu, \\sigma^2, w)$. \n",
        "  - Mean $\\mu$ can be initialized by the value of a randomly selected data. \n",
        "  - Variance $\\sigma^2$ can be initialized by the variance of 15 randomly selected data. \n",
        "  - Weight $w$ can be initialized by 1.\n",
        "\n",
        "- Step2: Go to an iteration\n",
        "\n",
        "  - Compute a score for each data point $x$ and all the clusters, using the provided *pdf* function.\n",
        "  - Assign each data point to the cluster with the highest score\n",
        "  - Compute normalized score of each data point by: $S_{i,k}=\\dfrac{s_{i,k}}{\\sum_{j=1}^{K}{s_{i,j}}}$ ($K$ is the number of clusters)\n",
        "  - For each cluster k \n",
        "    - Update mean of the cluster by: $\\mu_k'=\\dfrac{\\sum_{i=1}^{N}(x_iS_{i,k})}{0.001+\\sum_{i=1}^{N}{S_{i,k}}}$\n",
        "    - Update variance of the cluster by: $\\sigma_k'^2 = \\dfrac{\\sum_{i=1}^{N}[S_{i,k}(x_i - \\mu)^2]}{0.001+\\sum_{i=1}^{N}{S_{i,k}}}$\n",
        "    - Update weight of the cluster by: $\\dfrac{\\sum_{i=1}^{N}S_{i,k}}{N}$\n",
        "    - $N$ is the number of data points in the dataset\n",
        "    - $s_{i,k}$ is the score of the $i$-th data point for the cluster k\n",
        "\n",
        "### Expected output format:\n",
        "Each line describes a cluster including the cluster ID, mean, variance, and weight.\n",
        "```\n",
        "Cluster 1: mean=12.00, variance=1.23, weight=2.23\n",
        "Cluster 2: mean=5.23, variance=2.33, weight=4.23\n",
        "...\n",
        "Cluster 15: mean=17.20, variance=0.64, weight=1.68\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "piyrM_sOpYQ8"
      },
      "source": [
        "def pdf(data: float, mean: float, variance: float, weight: float):\n",
        "  s1 = 1/(np.sqrt(2*np.pi*variance))\n",
        "  s2 = np.exp(-(np.square(data - mean)/(2*variance)))\n",
        "  return s1 * s2 * weight"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_uVBRnuw5NBa"
      },
      "source": [
        "#Question_5\n",
        "\n",
        "#Fill out here"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
