{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Homework_1_Spr2022.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sq8U3BtmhtRx"
      },
      "source": [
        "\n",
        "# **Running Pyspark in Colab**\n",
        "PySpark is the Python API for Spark, which is an important tool in Big Data. To run spark in Colab, we need to first install all the dependencies in Colab environment i.e. Apache Spark 3.2.1 with hadoop 2.7, Java 8 and Findspark to locate the spark in the system. The tools installation can be carried out inside the Jupyter Notebook of the Colab. Follow the steps to install the dependencies:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v1b8k_OVf2QF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e035855b-f87e-49f0-d108-20fe673a1bce"
      },
      "source": [
        "!pip install pyspark"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.2.1.tar.gz (281.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 281.4 MB 32 kB/s \n",
            "\u001b[?25hCollecting py4j==0.10.9.3\n",
            "  Downloading py4j-0.10.9.3-py2.py3-none-any.whl (198 kB)\n",
            "\u001b[K     |████████████████████████████████| 198 kB 43.3 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.2.1-py2.py3-none-any.whl size=281853642 sha256=cf832bec88c432b8c847f55e79b9ad065c67bdc75c5447f2d54b9266559f45bf\n",
            "  Stored in directory: /root/.cache/pip/wheels/9f/f5/07/7cd8017084dce4e93e84e92efd1e1d5334db05f2e83bcef74f\n",
            "Successfully built pyspark\n",
            "Installing collected packages: py4j, pyspark\n",
            "Successfully installed py4j-0.10.9.3 pyspark-3.2.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KwrqMk3HiMiE"
      },
      "source": [
        "Run a local spark session to test your installation:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9_Uz1NL4gHFx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "83af365e-8f8d-493c-d184-334becbcf4fa"
      },
      "source": [
        "!pip install findspark\n",
        "import findspark\n",
        "findspark.init()\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting findspark\n",
            "  Downloading findspark-2.0.0-py2.py3-none-any.whl (4.4 kB)\n",
            "Installing collected packages: findspark\n",
            "Successfully installed findspark-2.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JEb4HTRwiaJx"
      },
      "source": [
        "Congrats! Your Colab is ready to run Pyspark.\n",
        "\n",
        "# Read input text file to RDD \n",
        "\n",
        "The first step to use Pyspark is reading input text file to resilient distributed dataset (RDD) provides by Spark as the primary abstraction.\n",
        "\n",
        "Download the input data from [here](https://raw.githubusercontent.com/umddm/ECE795_Homeworks/master/clust_B.txt) and keep it in the Colab document by the following command. The input data can also be downloaded by the link."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PAISFqHXf7dt"
      },
      "source": [
        "!wget https://raw.githubusercontent.com/umddm/ECE795_Homeworks/master/clust_B.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LNsM_jHqrjBg"
      },
      "source": [
        "Check the input data correctly in the system by the following command"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m606eNuQgA82",
        "outputId": "79e1c8b6-e781-426e-94fe-e56a6d23a782",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "!ls"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "clust_B.txt  spark-2.4.4-bin-hadoop2.7\n",
            "sample_data  spark-2.4.4-bin-hadoop2.7.tgz\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "21D9EANUvnwF"
      },
      "source": [
        "Now that we have input data, we can start to do the homework. \n",
        "\n",
        "## Question 1: Use sc.textFile to read the provided input data and split different fields.\n",
        "\n",
        "### Expected output of rdd.take(10):\n",
        "```\n",
        "['23785', '20305', 'i love you', '43']\n",
        "['7', '7', 'hate that i love you so', '36543']\n",
        "['1100', '453', 'hate that i love you', '81168']\n",
        "['6', '6', 'that i love you', '950238']\n",
        "['21041', '18574', 'yes we can', '1485']\n",
        "['308', '302', 'yes we can yes we can', '43112']\n",
        "['262', '249', 'yes we will', '175567']\n",
        "['6', '6', 'we can we will', '555912']\n",
        "['8', '8', 'yes we can and finally yes we will', '639024']\n",
        "['15', '15', 'yes we can and yes we will', '639122']\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ZeJ7WQCgM8g"
      },
      "source": [
        "#sc.textFile\n",
        "from pyspark import SparkConf, SparkContext\n",
        "sc = SparkContext.getOrCreate()\n",
        "\n",
        "#Question_1:\n",
        "\n",
        "#Fill out here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y3vYyp5dwOm_"
      },
      "source": [
        "## Question 2: Convert all the elements in the first and second columns of the RDD to integer type and save in \"Column1.txt\" and \"Column2.txt\".\n",
        "\n",
        "Please do not upload the generated output files to Blackboard"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eja1BLiaTThT"
      },
      "source": [
        "#Question_2\n",
        "\n",
        "#Fill out here\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6FHVHVQjd4P1"
      },
      "source": [
        "## Question 3: Please output the number of elements in the RDD with 'love', 'will' and 'university'.\n",
        "For example, a valid element can be 'love', 'I will', 'university of miami' or 'william' (will is a substring of the words)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MJsxix2Xd3-Q"
      },
      "source": [
        "#Question_3\n",
        "\n",
        "#Fill out here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uIy1IL965Ogm"
      },
      "source": [
        "## Question 4: Please utilize the following action RDD operation `sum` to compute how many times the word 'will' appears in the given input file.\n",
        "\n",
        "sum() is an action type of RDD operation, which generate the sum of all the elements in RDD.\n",
        "For example: If rdd includes elements of 1, 2, and 3, rdd.sum() will return 6 as the output.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7OmbLiCQ5ctu"
      },
      "source": [
        "#Question_4\n",
        "\n",
        "#Fill out here"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}